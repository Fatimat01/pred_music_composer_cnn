{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fatimat01/pred_music_composer_cnn/blob/Greg_Bauer/Project_Report_Team2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29c77d03-5275-4ec1-b889-31bbbb726e2f",
      "metadata": {
        "id": "29c77d03-5275-4ec1-b889-31bbbb726e2f"
      },
      "source": [
        "# Greg Bauer\n",
        "## Final Project - Team 2\n",
        "### AAI-511 Summer 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cc4e5384-aef2-4fa1-ba8c-fcc1108e3b39",
      "metadata": {
        "id": "cc4e5384-aef2-4fa1-ba8c-fcc1108e3b39",
        "outputId": "eaf8759f-0703-43d0-83bd-d34ca5a3aa46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install the 'pretty_midi' library: provides MIDI parsing, synthesis, and manipulation capabilities\n",
        "# Used for extracting note-level data, pitch, velocity, timing, and control events from .mid files\n",
        "!pip install pretty_midi --quiet\n",
        "\n",
        "# Upgrade pip to the latest version: ensures compatibility when installing newer Python packages\n",
        "# Especially useful for avoiding installation errors with dependencies like TensorFlow or librosa\n",
        "!python -m pip install --upgrade pip --quiet\n",
        "\n",
        "# Install 'librosa': a powerful library for audio signal processing and feature extraction\n",
        "# Useful if combining MIDI with audio-based workflows (e.g., MFCCs, spectral contrast, tempo estimation)\n",
        "!pip install librosa --quiet\n",
        "\n",
        "# Install 'midi2audio': allows you to convert symbolic MIDI data into actual audio (e.g., WAV format)\n",
        "# Enables playback, waveform analysis, or visualization (e.g., spectrograms) from MIDI inputs\n",
        "!pip install midi2audio --quiet\n",
        "\n",
        "# Install 'tensorflow': core deep learning framework for building and training models\n",
        "# Required for implementing CNN→LSTM and MLP architectures used in composer classification\n",
        "!pip install tensorflow --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d10fedc-d915-45e1-87e9-2a7c9c3330f0",
      "metadata": {
        "id": "9d10fedc-d915-45e1-87e9-2a7c9c3330f0"
      },
      "outputs": [],
      "source": [
        "# Core filesystem operations (e.g. loading MIDI files from directories)\n",
        "import os\n",
        "\n",
        "# Efficient array computation, mathematical operations, and broadcasting logic\n",
        "import numpy as np\n",
        "\n",
        "# Parses and manipulates symbolic music data from MIDI files\n",
        "import pretty_midi\n",
        "\n",
        "# Structured data handling for feature tables and model inputs/outputs\n",
        "import pandas as pd\n",
        "\n",
        "# Plotting utilities for statistical and heatmap-style visualizations\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt  # Standard tool for 2D plotting and axis control\n",
        "\n",
        "# Common mathematical functions for preprocessing and scoring\n",
        "import math\n",
        "\n",
        "# Recursive search to find files using wildcard patterns (e.g., all .mid files)\n",
        "from glob import glob\n",
        "\n",
        "# Deep learning model definition API used for building CNN → LSTM hybrid models\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Sequential and time-aware layers for hybrid architecture:\n",
        "# - Input: defines entry point and shape of input data\n",
        "# - TimeDistributed: applies CNN layers independently to each frame\n",
        "# - Conv2D: extracts spatial pitch patterns per time step\n",
        "# - MaxPooling2D: compresses learned features by reducing resolution\n",
        "# - Flatten: converts image-like frames into vectors\n",
        "# - LSTM: learns long-term musical dependencies and phrasing\n",
        "# - Dense: fully connected decision layers\n",
        "# - Dropout: helps generalize by randomly disabling units during training\n",
        "from tensorflow.keras.layers import Input, TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout\n",
        "\n",
        "# Label encoding and one-hot conversion for composer classification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Pads input sequences to equal length for uniform training batches\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Training control callbacks:\n",
        "# - EarlyStopping: halts training when validation loss plateaus\n",
        "# - ModelCheckpoint: saves model only when performance improves\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Splits dataset into training and validation sets with stratified label distribution\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53047d17-d5e9-440a-b745-5c162007be4a",
      "metadata": {
        "id": "53047d17-d5e9-440a-b745-5c162007be4a"
      },
      "outputs": [],
      "source": [
        "# Set a global debug flag: enables or disables verbose output across notebook\n",
        "DEBUG = False  # Toggle this to True when debugging\n",
        "\n",
        "# Suppress runtime warnings for cleaner notebook output when DEBUG is False\n",
        "# This avoids clutter from harmless numerical or audio-related warnings (e.g., librosa or pretty_midi)\n",
        "import warnings\n",
        "if not DEBUG:\n",
        "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c9417a8-325a-41d9-bb13-00796da4d386",
      "metadata": {
        "id": "8c9417a8-325a-41d9-bb13-00796da4d386"
      },
      "source": [
        "## What Are .mid Files?\n",
        "\n",
        "`.mid` files are the standard extension for **Musical Instrument Digital Interface (MIDI)** files. MIDI is a protocol that stores musical instructions rather than sound — enabling digital devices to communicate performance data.\n",
        "\n",
        "---\n",
        "\n",
        "### How MIDI Files Work\n",
        "\n",
        "A `.mid` file encodes structured event data, including:\n",
        "- Note pitches, start and end times, and velocities (volume)\n",
        "- Tempo and time signature changes\n",
        "- Instrument assignments and control signals\n",
        "\n",
        "These files are interpreted by synthesizers, software instruments, or music production tools to generate audio. Unlike audio recordings (e.g., .mp3 or .wav), `.mid` files are compact and easily editable.\n",
        "\n",
        "---\n",
        "\n",
        "### Why They're Used in Machine Learning\n",
        "\n",
        "- Small file size and efficient to process\n",
        "- Structured, symbolic representation ideal for extracting features like note density or chord distribution\n",
        "- Enables data augmentation through pitch and tempo manipulation\n",
        "\n",
        "---\n",
        "\n",
        "**Source:**  \n",
        "- [MIDI Technical Specification by the MIDI Manufacturers Association](https://www.midi.org/specifications)\n",
        "\n",
        "  ![image.png](attachment:5f7b6a18-698b-423f-8408-f2f513720def.png)\n",
        "\n",
        "### MIDI File Visualization Explained\n",
        "\n",
        "This graphic illustrates the structure of a MIDI file using a **piano roll representation**, a common way to visualize symbolic music data.\n",
        "\n",
        "### Grid Layout\n",
        "- **Horizontal Rows**: Represent musical pitches, labeled from **C2 to C5**.\n",
        "- **Vertical Columns**: Represent time intervals, evenly spaced to show rhythmic progression.\n",
        "\n",
        "### MIDI Notes\n",
        "- **Turquoise Bars**: Each bar corresponds to a MIDI note.\n",
        "  - **Vertical Position**: Indicates the pitch (e.g., C3, C4).\n",
        "  - **Horizontal Length**: Indicates the duration of the note.\n",
        "- **Example**:\n",
        "  - A long bar in the **C2** row spans nearly the entire width, suggesting a sustained bass note.\n",
        "  - Multiple shorter bars in **C3–C5** rows show melodic or harmonic activity.\n",
        "\n",
        "### Purpose\n",
        "This visualization helps:\n",
        "- Understand **note timing and pitch** at a glance.\n",
        "- Analyze **musical structure** for tasks like composer classification or genre detection.\n",
        "- Serve as input for deep learning models (e.g., CNN→LSTM) that process symbolic music data.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c457f7be-9830-43e1-913a-92c0deeb92ba",
      "metadata": {
        "id": "c457f7be-9830-43e1-913a-92c0deeb92ba"
      },
      "outputs": [],
      "source": [
        "file_path_dev = \"NN_midi_files_extended/dev/\"\n",
        "file_path_train = \"NN_midi_files_extended/train/\"\n",
        "file_path_test = \"NN_midi_files_extended/test/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b507118-f766-493a-92e7-3897c158aca2",
      "metadata": {
        "id": "9b507118-f766-493a-92e7-3897c158aca2"
      },
      "source": [
        "### 2. Data Pre-processing: Convert the musical scores into a format suitable for deep learning models. This involves converting the musical scores into MIDI files and applying data augmentation techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1241506-e897-41dc-ab51-ad16266ec7eb",
      "metadata": {
        "id": "c1241506-e897-41dc-ab51-ad16266ec7eb"
      },
      "outputs": [],
      "source": [
        "# Load a MIDI file and label it with its composer\n",
        "def load_midi(file_path):\n",
        "    try:\n",
        "        # Use PrettyMIDI to parse the file into an object for analysis and manipulation\n",
        "        # Source: https://craffel.github.io/PrettyMIDI\n",
        "        midi = pretty_midi.PrettyMIDI(file_path)\n",
        "        # Extract the composer name based on the parent directory structure\n",
        "        composer = os.path.basename(os.path.dirname(file_path))\n",
        "        return midi, composer\n",
        "    except Exception as e:\n",
        "        # If the file fails to load, report the issue and skip it\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Remove unwanted instruments to simplify downstream processing\n",
        "def sanitize_midi(midi):\n",
        "    # Keep only instruments with 'piano' in their name and remove all drum tracks\n",
        "    # This ensures stylistic consistency across inputs (e.g., modeling piano performance only)\n",
        "    # MIDI program definitions: https://www.midi.org/specifications-old/item/the-midi-1-0-specification\n",
        "    midi.instruments = [\n",
        "        inst for inst in midi.instruments\n",
        "        if not inst.is_drum and 'piano' in inst.name.lower()\n",
        "    ]\n",
        "    return midi\n",
        "\n",
        "# Remove expressive metadata that could introduce bias or inconsistencies\n",
        "def reassign_control_changes(midi):\n",
        "    # Control changes include parameters like volume, pan, modulation\n",
        "    # These are often arbitrary across MIDI files and may hurt model generalization\n",
        "    # Source: PrettyMIDI instrument API: https://craffel.github.io/PrettyMIDI/generated/pretty_midi.Instrument.html\n",
        "    for inst in midi.instruments:\n",
        "        inst.control_changes.clear()\n",
        "    return midi\n",
        "\n",
        "# Create data variations by simulating transpositions and tempo shifts\n",
        "def augment_midi(midi, pitch_shift=0, tempo_factor=1.0):\n",
        "    for inst in midi.instruments:\n",
        "        for note in inst.notes:\n",
        "            # Shift the pitch of each note up/down by pitch_shift semitones\n",
        "            # Clip to valid MIDI pitch range [0, 127]; Source: MIDI spec\n",
        "            note.pitch = np.clip(note.pitch + pitch_shift, 0, 127)\n",
        "            # Multiply start and end times to simulate different playing speeds\n",
        "            note.start *= tempo_factor\n",
        "            note.end *= tempo_factor\n",
        "    return midi\n",
        "\n",
        "# Recursively load, clean, and augment all MIDI files in a dataset\n",
        "def process_midi_directory(root_dir, pitch_shifts=[0, 2, -2], tempo_factors=[1.0, 1.1, 0.9]):\n",
        "    processed = []\n",
        "\n",
        "    # Find all .mid files inside composer subfolders using pattern matching\n",
        "    # Source: Python glob module — https://docs.python.org/3/library/glob.html\n",
        "    midi_files = glob(f\"{root_dir}/**/*.mid\", recursive=True)\n",
        "\n",
        "    for file_path in midi_files:\n",
        "        # Load and label the MIDI file\n",
        "        midi, composer = load_midi(file_path)\n",
        "        if not midi:\n",
        "            continue\n",
        "\n",
        "        # Filter instrument tracks and clean out control data\n",
        "        midi = sanitize_midi(midi)\n",
        "        midi = reassign_control_changes(midi)\n",
        "\n",
        "        # Apply combinations of pitch and tempo augmentation\n",
        "        for ps in pitch_shifts:\n",
        "            for tf in tempo_factors:\n",
        "                # Reload the original file to avoid compound modifications\n",
        "                midi_aug = augment_midi(pretty_midi.PrettyMIDI(file_path), pitch_shift=ps, tempo_factor=tf)\n",
        "\n",
        "                # Append the augmented sample and its metadata\n",
        "                processed.append({\n",
        "                    \"composer\": composer,                     # Source label\n",
        "                    \"file\": os.path.basename(file_path),      # Original filename\n",
        "                    \"pitch_shift\": ps,                        # Semitone shift applied\n",
        "                    \"tempo_factor\": tf,                       # Tempo scaling factor\n",
        "                    \"midi\": midi_aug                          # Resulting MIDI object\n",
        "                })\n",
        "\n",
        "    # Return the full list of processed samples\n",
        "    return processed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6708609-fd1c-4292-b2f6-158321b0fdc3",
      "metadata": {
        "id": "b6708609-fd1c-4292-b2f6-158321b0fdc3"
      },
      "outputs": [],
      "source": [
        "processed = process_midi_directory(file_path_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "884bb392-f5b6-4b9d-82fd-f5de25d70321",
      "metadata": {
        "id": "884bb392-f5b6-4b9d-82fd-f5de25d70321"
      },
      "source": [
        "### 3. Feature Extraction: Extract features from the MIDI files, such as notes, chords, and tempo, using music analysis tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39b63189-054f-498b-a1ec-c5c43425aa18",
      "metadata": {
        "id": "39b63189-054f-498b-a1ec-c5c43425aa18"
      },
      "outputs": [],
      "source": [
        "def midi_to_pianoroll_tensor(midi, fs=50, pitch_range=(21, 108), max_duration=30.0):\n",
        "    \"\"\"\n",
        "    Convert a PrettyMIDI object into a normalized piano roll tensor suitable for CNN→LSTM input.\n",
        "\n",
        "    Parameters:\n",
        "    - midi: PrettyMIDI object representing a parsed MIDI file.\n",
        "    - fs: Frame rate in Hz (frames per second). Lower values reduce temporal resolution and memory usage.\n",
        "    - pitch_range: Tuple defining the pitch bounds to extract (default is full piano range: A0 to C8).\n",
        "    - max_duration: Maximum duration (in seconds) to retain from the MIDI file. Longer files are truncated.\n",
        "\n",
        "    Returns:\n",
        "    - A 3D numpy array of shape (time_steps, pitch_bins, 1), normalized to [0, 1], ready for Conv2D layers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Truncate long MIDI files to avoid excessive memory usage\n",
        "    # This is critical because get_piano_roll allocates memory proportional to duration × fs\n",
        "    # We iterate over each instrument and remove notes that start after max_duration\n",
        "    for inst in midi.instruments:\n",
        "        inst.notes = [note for note in inst.notes if note.start < max_duration]\n",
        "\n",
        "    # Generate the piano roll matrix\n",
        "    # PrettyMIDI's get_piano_roll returns a (128, time_frames) matrix of velocities\n",
        "    # Each row corresponds to a MIDI pitch (0–127), each column to a time frame\n",
        "    # fs=50 means 20ms resolution, which balances temporal fidelity and memory efficiency\n",
        "    full_roll = midi.get_piano_roll(fs=fs)\n",
        "\n",
        "    # Restrict to standard piano pitch range\n",
        "    # This removes unused or irrelevant pitches (e.g., percussion, extreme registers)\n",
        "    # A0 (21) to C8 (108) covers 88 keys, but we use 87 bins: [21, 108)\n",
        "    roll = full_roll[pitch_range[0]:pitch_range[1], :]  # Shape: (87, time_frames)\n",
        "\n",
        "    # Transpose to time-major format\n",
        "    # Deep learning models typically expect input as (time_steps, features)\n",
        "    # Here, each time step contains a vector of pitch activations\n",
        "    roll = roll.T  # New shape: (time_steps, 87)\n",
        "\n",
        "    # Normalize velocity values to [0, 1]\n",
        "    # MIDI velocities range from 0 to 127; normalization ensures consistent scale\n",
        "    # If the roll is silent (all zeros), we avoid division by zero\n",
        "    roll = roll / 127.0 if roll.max() > 0 else np.zeros_like(roll)\n",
        "\n",
        "    # Add channel dimension for CNN compatibility\n",
        "    # Keras Conv2D expects input shape: (time_steps, pitch_bins, channels)\n",
        "    # We treat each time slice as a \"grayscale image\" with 1 channel\n",
        "    roll = np.expand_dims(roll, axis=-1)  # Final shape: (time_steps, 87, 1)\n",
        "\n",
        "    return roll\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e010adc-fdcb-47b6-b258-bc349fb43c29",
      "metadata": {
        "id": "8e010adc-fdcb-47b6-b258-bc349fb43c29"
      },
      "outputs": [],
      "source": [
        "# Create empty lists to hold:\n",
        "# - X: model inputs (each is a 3D piano roll tensor extracted from a MIDI file)\n",
        "# - y: target labels (composer names, one per tensor)\n",
        "X = []  # Feature set: piano roll tensors for each augmented MIDI file\n",
        "y = []  # Label set: composer identifiers (to be encoded)\n",
        "\n",
        "# Iterate over all processed and augmented MIDI samples\n",
        "for entry in processed:\n",
        "    # Convert MIDI data into a piano roll tensor\n",
        "    # Each tensor captures temporal pitch activation in the shape (time_steps, pitch_bins, 1 channel)\n",
        "    # These are suitable for TimeDistributed Conv2D layers followed by LSTM\n",
        "    # Reference: https://craffel.github.io/PrettyMIDI/generated/pretty_midi.PrettyMIDI.html#get_piano_roll\n",
        "    tensor = midi_to_pianoroll_tensor(entry['midi'])\n",
        "\n",
        "    # Add the tensor to training features\n",
        "    X.append(tensor)\n",
        "\n",
        "    # Add the associated composer label as the training target\n",
        "    # This label will be numerically encoded and one-hot transformed before model training\n",
        "    # Reference: Label encoding for classification — https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "    y.append(entry['composer'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98b638ff-465a-497e-a671-7b82ec0617ef",
      "metadata": {
        "id": "98b638ff-465a-497e-a671-7b82ec0617ef"
      },
      "source": [
        "### 4. Model Building: Develop a deep learning model using LSTM and CNN architectures to classify the musical scores according to the composer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e63bd635-e653-4313-bf86-59496b2c99c5",
      "metadata": {
        "id": "e63bd635-e653-4313-bf86-59496b2c99c5"
      },
      "outputs": [],
      "source": [
        "def build_cnn_lstm_model(input_shape, num_classes):\n",
        "    # Define input layer with shape: (time, pitch, channel)\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Apply CNN layers frame-by-frame using TimeDistributed\n",
        "    # Each frame is treated as a 2D \"image\" with pitch axis and channel dimension\n",
        "    x = TimeDistributed(Conv2D(32, (5, 1), activation='relu'))(inputs)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 1)))(x)\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "\n",
        "    # Feed sequence of CNN features into LSTM layer\n",
        "    # LSTM learns temporal dependencies across piano roll frames\n",
        "    x = LSTM(128)(x)\n",
        "\n",
        "    # Dropout helps prevent overfitting during training\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # Fully connected layer for deeper representation\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "\n",
        "    # Final output layer with softmax activation for classification\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define and compile the model\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',  # suitable for multi-class classification\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f82a154-943a-44f9-9411-22c49b158e19",
      "metadata": {
        "id": "7f82a154-943a-44f9-9411-22c49b158e19"
      },
      "outputs": [],
      "source": [
        "# Convert composer names into integer class labels\n",
        "# LabelEncoder assigns a unique integer to each composer name\n",
        "# Source: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)  # Integer labels: [0, 1, 2, ...]\n",
        "\n",
        "# Convert integer labels into one-hot vectors\n",
        "# This is required by softmax output and categorical_crossentropy loss\n",
        "# Source: https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical\n",
        "y_onehot = to_categorical(y_encoded)\n",
        "\n",
        "# You can also access the class names like this:\n",
        "# class_names = label_encoder.classes_\n",
        "\n",
        "# Now y_onehot is your final target array for model training\n",
        "# Each label is a vector of length equal to the number of composers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5922ce9d-3f34-40aa-a11e-146d4d0cb350",
      "metadata": {
        "id": "5922ce9d-3f34-40aa-a11e-146d4d0cb350",
        "outputId": "8f6b2659-e6ce-468e-da07-68ad1756843d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3321, 4000, 87, 1)\n"
          ]
        }
      ],
      "source": [
        "# First, convert X to a NumPy array of objects (since tensors may vary in shape)\n",
        "X_padded = pad_sequences(X, maxlen=4000, padding='post', dtype='float32')\n",
        "\n",
        "print(X_padded.shape)  # Should be (num_samples, max_time, pitch_bins, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d21faa-fd58-4a13-9b0c-3d5cb319c0a5",
      "metadata": {
        "id": "a6d21faa-fd58-4a13-9b0c-3d5cb319c0a5"
      },
      "outputs": [],
      "source": [
        "# After padding sequences, expand the tensor to add a second spatial dimension\n",
        "# Original shape: (samples, time_steps, pitch_bins, 1)\n",
        "# New shape:      (samples, time_steps, pitch_bins, 1, 1)\n",
        "# This ensures compatibility with Conv2D inside TimeDistributed\n",
        "X_reshaped = np.expand_dims(X_padded, axis=-1)\n",
        "\n",
        "# Split the reshaped input data and one-hot encoded labels into training and validation sets\n",
        "# - 'stratify': ensures class distribution stays balanced across both sets\n",
        "# - 'test_size': allocates 20% of data for validation\n",
        "# - 'random_state': ensures reproducibility of the split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_reshaped, y_onehot,\n",
        "    stratify=y_encoded,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Extract input dimensions from a single training sample\n",
        "# These dimensions are passed into the model to define the input layer shape\n",
        "time_dim, pitch_dim, width_dim, channel_dim = X_train.shape[1:]\n",
        "\n",
        "# Build the CNN→LSTM hybrid model using the computed input shape and number of classes\n",
        "# - 'input_shape': defines the structure of each input sample (per time step frame)\n",
        "# - 'num_classes': sets the output dimension for softmax classification\n",
        "model = build_cnn_lstm_model(\n",
        "    input_shape=(time_dim, pitch_dim, width_dim, channel_dim),\n",
        "    num_classes=len(label_encoder.classes_)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd7cfeef-963d-43a0-95ce-893ea1c11d25",
      "metadata": {
        "id": "cd7cfeef-963d-43a0-95ce-893ea1c11d25",
        "outputId": "17eaf343-7793-4540-869f-b4a387caef0a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">87</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>,    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                    │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>,    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                    │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1312</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">737,792</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">585</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m, \u001b[38;5;34m87\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m, \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m1\u001b[0m,    │           \u001b[38;5;34m192\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │ \u001b[38;5;34m32\u001b[0m)                    │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m1\u001b[0m,    │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │ \u001b[38;5;34m32\u001b[0m)                    │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m, \u001b[38;5;34m1312\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m737,792\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │           \u001b[38;5;34m585\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">746,825</span> (2.85 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m746,825\u001b[0m (2.85 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">746,825</span> (2.85 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m746,825\u001b[0m (2.85 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "716f9c9f-468d-4fd7-a403-3cc3971f0881",
      "metadata": {
        "id": "716f9c9f-468d-4fd7-a403-3cc3971f0881"
      },
      "source": [
        "### **Model Architecture Explained in Plain Terms**\n",
        "\n",
        "This model is built to listen to music (as a piano roll) and figure out which composer might have written it. It does this step by step, analyzing each slice of the music and learning how musical ideas evolve over time.\n",
        "\n",
        "#### **Layer Breakdown and Their Roles**\n",
        "\n",
        "| **Layer**                  | **What It Means (In Plain Terms)**                                                                 |\n",
        "|---------------------------|-----------------------------------------------------------------------------------------------------|\n",
        "| `TimeDistributed(Conv2D)` | Treats each moment of the music like a mini image and applies 32 pattern detectors to find motifs. It scans vertically across pitches to detect things like chords, intervals, or melodic contours. |\n",
        "| `MaxPooling2D((2,1))`     | Simplifies the detected patterns by reducing pitch resolution—like shrinking the image—while keeping essential information. This helps the model focus on broad musical shapes. |\n",
        "| `Flatten`                 | Converts each compressed musical slice into a row of numbers that the model can understand and learn from. This step prepares each moment for sequence learning. |\n",
        "| `LSTM(128)`               | Acts like musical memory—it learns how these patterns change over time, allowing the model to recognize phrasing, repetition, or rhythmic flow across the piece. |\n",
        "| `Dropout(0.3)`            | A regularization trick that randomly turns off some neurons during training to prevent the model from memorizing noise. Think of it like practicing blindfolded to improve intuition. |\n",
        "| `Dense(64)`               | Combines learned ideas into higher-level insights before making a prediction. It’s a final interpretation layer that distills what's been learned. |\n",
        "| `Dense(9)`                | Outputs 9 numbers that represent how likely the piece belongs to each composer. The highest number points to the model’s best guess. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Additional Stats and What They Tell Us**\n",
        "\n",
        "- **Parameter Count**: ~747,000 → This is how many things the model can tune as it learns. It's efficient enough to train quickly but expressive enough to capture real musical structure.\n",
        "- **Features After Flattening**: 1,312 values per moment in time → These represent all the musical information pulled from the convolutional layers, giving the LSTM plenty of texture to learn from.\n",
        "- **Output Shape**: `(None, 9)` → For every music sample, the model gives a vector of 9 scores, one for each composer. It picks the one with the highest score as its prediction.\n",
        "\n",
        "---\n",
        "\n",
        "This architecture combines visual pattern recognition (like reading sheet music) with temporal listening (like following a melody), making it especially well-suited for composer classification tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4794009c-7293-45c5-b681-d247a6a830fd",
      "metadata": {
        "id": "4794009c-7293-45c5-b681-d247a6a830fd"
      },
      "source": [
        "### 5. Model Training: Train the deep learning model using the pre-processed and feature-extracted data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3d20ae-deec-4131-9643-39aeba03d537",
      "metadata": {
        "id": "1a3d20ae-deec-4131-9643-39aeba03d537",
        "outputId": "f7228380-4b06-4112-b922-14b2280fe41a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available GPUs: []\n",
            "⚠️ No GPU detected — training will run on CPU\n",
            "Mixed precision policy: <DTypePolicy \"mixed_float16\">\n",
            "TensorBoard logging to: logs/fit/20250717-150231\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# TRAINING SETUP WITH CONFIGURABLE FEATURES\n",
        "# ==========================================\n",
        "\n",
        "# Toggle switches for enabling/disabling optional training features\n",
        "USE_GPU_CHECK        = True   # Check for GPU and report availability\n",
        "USE_MIXED_PRECISION  = True   # Use float16 operations to boost training speed on compatible GPUs\n",
        "USE_TENSORBOARD      = True   # Log training metrics and histograms for monitoring in TensorBoard\n",
        "USE_LR_SCHEDULER     = True   # Adjust learning rate dynamically to improve convergence and stability\n",
        "AUTO_SCALE_BATCH     = True   # Increase batch size automatically if GPU is available\n",
        "\n",
        "# ------------------------------------------\n",
        "# Device Check: Report available GPUs\n",
        "# ------------------------------------------\n",
        "if USE_GPU_CHECK:\n",
        "    from tensorflow.config import list_physical_devices\n",
        "    gpus = list_physical_devices('GPU')\n",
        "    print(\"Available GPUs:\", gpus)\n",
        "    if not gpus:\n",
        "        print(\"⚠️ No GPU detected — training will run on CPU\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# Mixed Precision: Enable float16 operations\n",
        "# Reduces memory footprint and speeds up computation\n",
        "# Recommended for Volta/Turing/Ampere GPUs (e.g. RTX series)\n",
        "# ------------------------------------------\n",
        "if USE_MIXED_PRECISION:\n",
        "    from tensorflow.keras import mixed_precision\n",
        "    mixed_precision.set_global_policy('mixed_float16')\n",
        "    print(\"Mixed precision policy:\", mixed_precision.global_policy())\n",
        "\n",
        "# ------------------------------------------\n",
        "# TensorBoard: Real-time training dashboard\n",
        "# View metrics at http://localhost:6006 after running:\n",
        "# tensorboard --logdir logs/fit/\n",
        "# ------------------------------------------\n",
        "if USE_TENSORBOARD:\n",
        "    import datetime\n",
        "    from tensorflow.keras.callbacks import TensorBoard\n",
        "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_cb = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "    print(\"TensorBoard logging to:\", log_dir)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Learning Rate Scheduler: Reduce LR every 10 epochs\n",
        "# Helps model fine-tune toward convergence after initial progress\n",
        "# ------------------------------------------\n",
        "if USE_LR_SCHEDULER:\n",
        "    from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "    def lr_schedule(epoch, lr):\n",
        "        if epoch % 10 == 0 and epoch != 0:\n",
        "            return lr * 0.5\n",
        "        return lr\n",
        "    lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Core Keras Callbacks\n",
        "# EarlyStopping prevents wasteful training beyond optimal point\n",
        "# ModelCheckpoint saves best weights based on validation performance\n",
        "# ------------------------------------------\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',           # Monitor validation loss to prevent overfitting\n",
        "    patience=5,                   # Wait up to 5 epochs for improvement\n",
        "    restore_best_weights=True     # Revert to best weights when training ends\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath='cnn_lstm_best_model.h5',  # Path to save best model\n",
        "    save_best_only=True                 # Save only if model improves\n",
        ")\n",
        "\n",
        "# ------------------------------------------\n",
        "# Auto-scaling batch size based on GPU availability\n",
        "# Larger batches improve training throughput when enough memory is available\n",
        "# ------------------------------------------\n",
        "batch_size = 32\n",
        "if AUTO_SCALE_BATCH and USE_GPU_CHECK and gpus:\n",
        "    batch_size = 64\n",
        "    print(\"Batch size auto-scaled to:\", batch_size)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Assemble final callback list based on active features\n",
        "# ------------------------------------------\n",
        "callbacks = [early_stop, checkpoint]\n",
        "if USE_TENSORBOARD:\n",
        "    callbacks.append(tensorboard_cb)\n",
        "if USE_LR_SCHEDULER:\n",
        "    callbacks.append(lr_scheduler)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Model Training: Fit CNN→LSTM on piano roll tensors\n",
        "# ------------------------------------------\n",
        "# - 'epochs': upper bound for training duration\n",
        "# - 'batch_size': number of samples used per training step\n",
        "# - 'validation_data': separate dataset used to evaluate generalization\n",
        "# - 'callbacks': applies monitoring, logging, regularization, and adaptive control\n",
        "# - 'verbose': controls verbosity of training output (1 = per epoch update)\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=30,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34bccd0a-cfcb-4ab6-8da6-3c157ccdea48",
      "metadata": {
        "id": "34bccd0a-cfcb-4ab6-8da6-3c157ccdea48"
      },
      "outputs": [],
      "source": [
        "tensorboard --logdir=logs/fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c99b7db5-47df-48f1-b176-ddf72739e8dd",
      "metadata": {
        "id": "c99b7db5-47df-48f1-b176-ddf72739e8dd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('CNN→LSTM Training Curve')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2326ad11-351f-4ff5-a9bf-eaa18efa8ae9",
      "metadata": {
        "id": "2326ad11-351f-4ff5-a9bf-eaa18efa8ae9"
      },
      "source": [
        "### 6. Model Evaluation: Evaluate the performance of the deep learning model using accuracy, precision, and recall metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5e106c9-22ab-4c12-94e4-71c0cd66c108",
      "metadata": {
        "id": "a5e106c9-22ab-4c12-94e4-71c0cd66c108"
      },
      "source": [
        "### 7. Model Optimization: Optimize the deep learning model by fine-tuning hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7a874ad-0718-4d2b-867e-c95abbe4d219",
      "metadata": {
        "id": "a7a874ad-0718-4d2b-867e-c95abbe4d219"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8898b1e7-a5ee-4a93-8541-f8da13e82546",
      "metadata": {
        "id": "8898b1e7-a5ee-4a93-8541-f8da13e82546"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}