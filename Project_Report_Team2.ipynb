{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fatimat01/pred_music_composer_cnn/blob/Greg_Bauer/Project_Report_Team2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29c77d03-5275-4ec1-b889-31bbbb726e2f",
      "metadata": {
        "id": "29c77d03-5275-4ec1-b889-31bbbb726e2f"
      },
      "source": [
        "# Greg Bauer\n",
        "## Final Project - Team 2\n",
        "### AAI-511 Summer 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cc4e5384-aef2-4fa1-ba8c-fcc1108e3b39",
      "metadata": {
        "id": "cc4e5384-aef2-4fa1-ba8c-fcc1108e3b39",
        "outputId": "362e6783-a9bf-4424-813b-808d6910390e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pretty_midi in /usr/local/lib/python3.11/dist-packages (0.2.10)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (2.0.2)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mido>=1.1.16->pretty_midi) (25.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Install the 'pretty_midi' library: provides MIDI parsing, synthesis, and manipulation capabilities\n",
        "# Used for extracting note-level data, pitch, velocity, timing, and control events from .mid files\n",
        "!pip install pretty_midi --quiet\n",
        "\n",
        "# Upgrade pip to the latest version: ensures compatibility when installing newer Python packages\n",
        "# Especially useful for avoiding installation errors with dependencies like TensorFlow or librosa\n",
        "#!python -m pip install --upgrade pip --quiet\n",
        "\n",
        "# Install 'librosa': a powerful library for audio signal processing and feature extraction\n",
        "# Useful if combining MIDI with audio-based workflows (e.g., MFCCs, spectral contrast, tempo estimation)\n",
        "#!pip install librosa --quiet\n",
        "\n",
        "# Install 'midi2audio': allows you to convert symbolic MIDI data into actual audio (e.g., WAV format)\n",
        "# Enables playback, waveform analysis, or visualization (e.g., spectrograms) from MIDI inputs\n",
        "#!pip install midi2audio --quiet\n",
        "\n",
        "# Install 'tensorflow': core deep learning framework for building and training models\n",
        "# Required for implementing CNN→LSTM and MLP architectures used in composer classification\n",
        "!pip install tensorflow --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9d10fedc-d915-45e1-87e9-2a7c9c3330f0",
      "metadata": {
        "id": "9d10fedc-d915-45e1-87e9-2a7c9c3330f0"
      },
      "outputs": [],
      "source": [
        "# Core filesystem operations (e.g. loading MIDI files from directories)\n",
        "import os\n",
        "\n",
        "# Efficient array computation, mathematical operations, and broadcasting logic\n",
        "import numpy as np\n",
        "\n",
        "# Parses and manipulates symbolic music data from MIDI files\n",
        "import pretty_midi\n",
        "\n",
        "# Structured data handling for feature tables and model inputs/outputs\n",
        "import pandas as pd\n",
        "\n",
        "# Plotting utilities for statistical and heatmap-style visualizations\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt  # Standard tool for 2D plotting and axis control\n",
        "\n",
        "# Common mathematical functions for preprocessing and scoring\n",
        "import math\n",
        "\n",
        "# Recursive search to find files using wildcard patterns (e.g., all .mid files)\n",
        "from glob import glob\n",
        "\n",
        "# Deep learning model definition API used for building CNN → LSTM hybrid models\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Sequential and time-aware layers for hybrid architecture:\n",
        "# - Input: defines entry point and shape of input data\n",
        "# - TimeDistributed: applies CNN layers independently to each frame\n",
        "# - Conv2D: extracts spatial pitch patterns per time step\n",
        "# - MaxPooling2D: compresses learned features by reducing resolution\n",
        "# - Flatten: converts image-like frames into vectors\n",
        "# - LSTM: learns long-term musical dependencies and phrasing\n",
        "# - Dense: fully connected decision layers\n",
        "# - Dropout: helps generalize by randomly disabling units during training\n",
        "from tensorflow.keras.layers import Input, TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout\n",
        "\n",
        "# Label encoding and one-hot conversion for composer classification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Pads input sequences to equal length for uniform training batches\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Training control callbacks:\n",
        "# - EarlyStopping: halts training when validation loss plateaus\n",
        "# - ModelCheckpoint: saves model only when performance improves\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Splits dataset into training and validation sets with stratified label distribution\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"NN_midi_files_extended.zip\"\n",
        "extract_path = \"NN_midi_files_extended\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"✅ Extraction complete.\")\n"
      ],
      "metadata": {
        "id": "_kxqOZbsZvbk",
        "outputId": "3118923a-df29-4aba-d065-aa10c1c2c95f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "id": "_kxqOZbsZvbk",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5519cd5d-9a86-493c-8fb4-62405e653dc5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5519cd5d-9a86-493c-8fb4-62405e653dc5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-1037851166.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53047d17-d5e9-440a-b745-5c162007be4a",
      "metadata": {
        "id": "53047d17-d5e9-440a-b745-5c162007be4a"
      },
      "outputs": [],
      "source": [
        "# Set a global debug flag: enables or disables verbose output across notebook\n",
        "DEBUG = False  # Toggle this to True when debugging\n",
        "\n",
        "# Suppress runtime warnings for cleaner notebook output when DEBUG is False\n",
        "# This avoids clutter from harmless numerical or audio-related warnings (e.g., librosa or pretty_midi)\n",
        "import warnings\n",
        "if not DEBUG:\n",
        "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c9417a8-325a-41d9-bb13-00796da4d386",
      "metadata": {
        "id": "8c9417a8-325a-41d9-bb13-00796da4d386"
      },
      "source": [
        "## What Are .mid Files?\n",
        "\n",
        "`.mid` files are the standard extension for **Musical Instrument Digital Interface (MIDI)** files. MIDI is a protocol that stores musical instructions rather than sound — enabling digital devices to communicate performance data.\n",
        "\n",
        "---\n",
        "\n",
        "### How MIDI Files Work\n",
        "\n",
        "A `.mid` file encodes structured event data, including:\n",
        "- Note pitches, start and end times, and velocities (volume)\n",
        "- Tempo and time signature changes\n",
        "- Instrument assignments and control signals\n",
        "\n",
        "These files are interpreted by synthesizers, software instruments, or music production tools to generate audio. Unlike audio recordings (e.g., .mp3 or .wav), `.mid` files are compact and easily editable.\n",
        "\n",
        "---\n",
        "\n",
        "### Why They're Used in Machine Learning\n",
        "\n",
        "- Small file size and efficient to process\n",
        "- Structured, symbolic representation ideal for extracting features like note density or chord distribution\n",
        "- Enables data augmentation through pitch and tempo manipulation\n",
        "\n",
        "---\n",
        "\n",
        "**Source:**  \n",
        "- [MIDI Technical Specification by the MIDI Manufacturers Association](https://www.midi.org/specifications)\n",
        "\n",
        "  ![image.png](attachment:5f7b6a18-698b-423f-8408-f2f513720def.png)\n",
        "\n",
        "### MIDI File Visualization Explained\n",
        "\n",
        "This graphic illustrates the structure of a MIDI file using a **piano roll representation**, a common way to visualize symbolic music data.\n",
        "\n",
        "### Grid Layout\n",
        "- **Horizontal Rows**: Represent musical pitches, labeled from **C2 to C5**.\n",
        "- **Vertical Columns**: Represent time intervals, evenly spaced to show rhythmic progression.\n",
        "\n",
        "### MIDI Notes\n",
        "- **Turquoise Bars**: Each bar corresponds to a MIDI note.\n",
        "  - **Vertical Position**: Indicates the pitch (e.g., C3, C4).\n",
        "  - **Horizontal Length**: Indicates the duration of the note.\n",
        "- **Example**:\n",
        "  - A long bar in the **C2** row spans nearly the entire width, suggesting a sustained bass note.\n",
        "  - Multiple shorter bars in **C3–C5** rows show melodic or harmonic activity.\n",
        "\n",
        "### Purpose\n",
        "This visualization helps:\n",
        "- Understand **note timing and pitch** at a glance.\n",
        "- Analyze **musical structure** for tasks like composer classification or genre detection.\n",
        "- Serve as input for deep learning models (e.g., CNN→LSTM) that process symbolic music data.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c457f7be-9830-43e1-913a-92c0deeb92ba",
      "metadata": {
        "id": "c457f7be-9830-43e1-913a-92c0deeb92ba"
      },
      "outputs": [],
      "source": [
        "file_path_dev = \"NN_midi_files_extended/NN_midi_files_extended/dev/\"\n",
        "file_path_train = \"NN_midi_files_extended/NN_midi_files_extended/train/\"\n",
        "file_path_test = \"NN_midi_files_extended/NN_midi_files_extended/test/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b507118-f766-493a-92e7-3897c158aca2",
      "metadata": {
        "id": "9b507118-f766-493a-92e7-3897c158aca2"
      },
      "source": [
        "### 2. Data Pre-processing: Convert the musical scores into a format suitable for deep learning models. This involves converting the musical scores into MIDI files and applying data augmentation techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1241506-e897-41dc-ab51-ad16266ec7eb",
      "metadata": {
        "id": "c1241506-e897-41dc-ab51-ad16266ec7eb"
      },
      "outputs": [],
      "source": [
        "# Load a MIDI file and label it with its composer\n",
        "def load_midi(file_path):\n",
        "    try:\n",
        "        # Use PrettyMIDI to parse the file into an object for analysis and manipulation\n",
        "        # Source: https://craffel.github.io/PrettyMIDI\n",
        "        midi = pretty_midi.PrettyMIDI(file_path)\n",
        "        # Extract the composer name based on the parent directory structure\n",
        "        composer = os.path.basename(os.path.dirname(file_path))\n",
        "        return midi, composer\n",
        "    except Exception as e:\n",
        "        # If the file fails to load, report the issue and skip it\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Remove unwanted instruments to simplify downstream processing\n",
        "def sanitize_midi(midi):\n",
        "    # Keep only instruments with 'piano' in their name and remove all drum tracks\n",
        "    # This ensures stylistic consistency across inputs (e.g., modeling piano performance only)\n",
        "    # MIDI program definitions: https://www.midi.org/specifications-old/item/the-midi-1-0-specification\n",
        "    midi.instruments = [\n",
        "        inst for inst in midi.instruments\n",
        "        if not inst.is_drum and 'piano' in inst.name.lower()\n",
        "    ]\n",
        "    return midi\n",
        "\n",
        "# Remove expressive metadata that could introduce bias or inconsistencies\n",
        "def reassign_control_changes(midi):\n",
        "    # Control changes include parameters like volume, pan, modulation\n",
        "    # These are often arbitrary across MIDI files and may hurt model generalization\n",
        "    # Source: PrettyMIDI instrument API: https://craffel.github.io/PrettyMIDI/generated/pretty_midi.Instrument.html\n",
        "    for inst in midi.instruments:\n",
        "        inst.control_changes.clear()\n",
        "    return midi\n",
        "\n",
        "# Create data variations by simulating transpositions and tempo shifts\n",
        "def augment_midi(midi, pitch_shift=0, tempo_factor=1.0):\n",
        "    for inst in midi.instruments:\n",
        "        for note in inst.notes:\n",
        "            # Shift the pitch of each note up/down by pitch_shift semitones\n",
        "            # Clip to valid MIDI pitch range [0, 127]; Source: MIDI spec\n",
        "            note.pitch = np.clip(note.pitch + pitch_shift, 0, 127)\n",
        "            # Multiply start and end times to simulate different playing speeds\n",
        "            note.start *= tempo_factor\n",
        "            note.end *= tempo_factor\n",
        "    return midi\n",
        "\n",
        "# Recursively load, clean, and augment all MIDI files in a dataset\n",
        "def process_midi_directory(root_dir, pitch_shifts=[0, 2, -2], tempo_factors=[1.0, 1.1, 0.9]):\n",
        "    processed = []\n",
        "\n",
        "    # Find all .mid files inside composer subfolders using pattern matching\n",
        "    # Source: Python glob module — https://docs.python.org/3/library/glob.html\n",
        "    midi_files = glob(f\"{root_dir}/**/*.mid\", recursive=True)\n",
        "\n",
        "    for file_path in midi_files:\n",
        "        # Load and label the MIDI file\n",
        "        midi, composer = load_midi(file_path)\n",
        "        if not midi:\n",
        "            continue\n",
        "\n",
        "        # Filter instrument tracks and clean out control data\n",
        "        midi = sanitize_midi(midi)\n",
        "        midi = reassign_control_changes(midi)\n",
        "\n",
        "        # Apply combinations of pitch and tempo augmentation\n",
        "        for ps in pitch_shifts:\n",
        "            for tf in tempo_factors:\n",
        "                # Reload the original file to avoid compound modifications\n",
        "                midi_aug = augment_midi(pretty_midi.PrettyMIDI(file_path), pitch_shift=ps, tempo_factor=tf)\n",
        "\n",
        "                # Append the augmented sample and its metadata\n",
        "                processed.append({\n",
        "                    \"composer\": composer,                     # Source label\n",
        "                    \"file\": os.path.basename(file_path),      # Original filename\n",
        "                    \"pitch_shift\": ps,                        # Semitone shift applied\n",
        "                    \"tempo_factor\": tf,                       # Tempo scaling factor\n",
        "                    \"midi\": midi_aug                          # Resulting MIDI object\n",
        "                })\n",
        "\n",
        "    # Return the full list of processed samples\n",
        "    return processed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6708609-fd1c-4292-b2f6-158321b0fdc3",
      "metadata": {
        "id": "b6708609-fd1c-4292-b2f6-158321b0fdc3"
      },
      "outputs": [],
      "source": [
        "processed = process_midi_directory(file_path_train)\n",
        "print(f\"Number of MIDI files processed: {len(processed)}\")\n",
        "print(f\"Example composer: {processed[0]['composer']}\")\n",
        "print(f\"Example file: {processed[0]['file']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "884bb392-f5b6-4b9d-82fd-f5de25d70321",
      "metadata": {
        "id": "884bb392-f5b6-4b9d-82fd-f5de25d70321"
      },
      "source": [
        "### 3. Feature Extraction: Extract features from the MIDI files, such as notes, chords, and tempo, using music analysis tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39b63189-054f-498b-a1ec-c5c43425aa18",
      "metadata": {
        "id": "39b63189-054f-498b-a1ec-c5c43425aa18"
      },
      "outputs": [],
      "source": [
        "def midi_to_pianoroll_tensor(midi, fs=50, pitch_range=(21, 108), max_duration=30.0):\n",
        "    \"\"\"\n",
        "    Convert a PrettyMIDI object into a normalized piano roll tensor suitable for CNN→LSTM input.\n",
        "\n",
        "    Parameters:\n",
        "    - midi: PrettyMIDI object representing a parsed MIDI file.\n",
        "    - fs: Frame rate in Hz (frames per second). Lower values reduce temporal resolution and memory usage.\n",
        "    - pitch_range: Tuple defining the pitch bounds to extract (default is full piano range: A0 to C8).\n",
        "    - max_duration: Maximum duration (in seconds) to retain from the MIDI file. Longer files are truncated.\n",
        "\n",
        "    Returns:\n",
        "    - A 3D numpy array of shape (time_steps, pitch_bins, 1), normalized to [0, 1], ready for Conv2D layers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Truncate long MIDI files to avoid excessive memory usage\n",
        "    # This is critical because get_piano_roll allocates memory proportional to duration × fs\n",
        "    # We iterate over each instrument and remove notes that start after max_duration\n",
        "    for inst in midi.instruments:\n",
        "        inst.notes = [note for note in inst.notes if note.start < max_duration]\n",
        "\n",
        "    # Generate the piano roll matrix\n",
        "    # PrettyMIDI's get_piano_roll returns a (128, time_frames) matrix of velocities\n",
        "    # Each row corresponds to a MIDI pitch (0–127), each column to a time frame\n",
        "    # fs=50 means 20ms resolution, which balances temporal fidelity and memory efficiency\n",
        "    full_roll = midi.get_piano_roll(fs=fs)\n",
        "\n",
        "    # Restrict to standard piano pitch range\n",
        "    # This removes unused or irrelevant pitches (e.g., percussion, extreme registers)\n",
        "    # A0 (21) to C8 (108) covers 88 keys, but we use 87 bins: [21, 108)\n",
        "    roll = full_roll[pitch_range[0]:pitch_range[1], :]  # Shape: (87, time_frames)\n",
        "\n",
        "    # Transpose to time-major format\n",
        "    # Deep learning models typically expect input as (time_steps, features)\n",
        "    # Here, each time step contains a vector of pitch activations\n",
        "    roll = roll.T  # New shape: (time_steps, 87)\n",
        "\n",
        "    # Normalize velocity values to [0, 1]\n",
        "    # MIDI velocities range from 0 to 127; normalization ensures consistent scale\n",
        "    # If the roll is silent (all zeros), we avoid division by zero\n",
        "    roll = roll / 127.0 if roll.max() > 0 else np.zeros_like(roll)\n",
        "\n",
        "    # Add channel dimension for CNN compatibility\n",
        "    # Keras Conv2D expects input shape: (time_steps, pitch_bins, channels)\n",
        "    # We treat each time slice as a \"grayscale image\" with 1 channel\n",
        "    roll = np.expand_dims(roll, axis=-1)  # Final shape: (time_steps, 87, 1)\n",
        "\n",
        "    return roll\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e010adc-fdcb-47b6-b258-bc349fb43c29",
      "metadata": {
        "id": "8e010adc-fdcb-47b6-b258-bc349fb43c29"
      },
      "outputs": [],
      "source": [
        "# Create empty lists to hold:\n",
        "# - X: model inputs (each is a 3D piano roll tensor extracted from a MIDI file)\n",
        "# - y: target labels (composer names, one per tensor)\n",
        "X = []  # Feature set: piano roll tensors for each augmented MIDI file\n",
        "y = []  # Label set: composer identifiers (to be encoded)\n",
        "\n",
        "# Iterate over all processed and augmented MIDI samples\n",
        "for entry in processed:\n",
        "    # Convert MIDI data into a piano roll tensor\n",
        "    # Each tensor captures temporal pitch activation in the shape (time_steps, pitch_bins, 1 channel)\n",
        "    # These are suitable for TimeDistributed Conv2D layers followed by LSTM\n",
        "    # Reference: https://craffel.github.io/PrettyMIDI/generated/pretty_midi.PrettyMIDI.html#get_piano_roll\n",
        "    tensor = midi_to_pianoroll_tensor(entry['midi'])\n",
        "\n",
        "    # Add the tensor to training features\n",
        "    X.append(tensor)\n",
        "\n",
        "    # Add the associated composer label as the training target\n",
        "    # This label will be numerically encoded and one-hot transformed before model training\n",
        "    # Reference: Label encoding for classification — https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "    y.append(entry['composer'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98b638ff-465a-497e-a671-7b82ec0617ef",
      "metadata": {
        "id": "98b638ff-465a-497e-a671-7b82ec0617ef"
      },
      "source": [
        "### 4. Model Building: Develop a deep learning model using LSTM and CNN architectures to classify the musical scores according to the composer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e63bd635-e653-4313-bf86-59496b2c99c5",
      "metadata": {
        "id": "e63bd635-e653-4313-bf86-59496b2c99c5"
      },
      "outputs": [],
      "source": [
        "def build_cnn_lstm_model(input_shape, num_classes):\n",
        "    # Define input layer with shape: (time, pitch, channel)\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Apply CNN layers frame-by-frame using TimeDistributed\n",
        "    # Each frame is treated as a 2D \"image\" with pitch axis and channel dimension\n",
        "    x = TimeDistributed(Conv2D(32, (5, 1), activation='relu'))(inputs)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 1)))(x)\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "\n",
        "    # Feed sequence of CNN features into LSTM layer\n",
        "    # LSTM learns temporal dependencies across piano roll frames\n",
        "    x = LSTM(128)(x)\n",
        "\n",
        "    # Dropout helps prevent overfitting during training\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # Fully connected layer for deeper representation\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "\n",
        "    # Final output layer with softmax activation for classification\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define and compile the model\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',  # suitable for multi-class classification\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f82a154-943a-44f9-9411-22c49b158e19",
      "metadata": {
        "id": "7f82a154-943a-44f9-9411-22c49b158e19"
      },
      "outputs": [],
      "source": [
        "# Convert composer names into integer class labels\n",
        "# LabelEncoder assigns a unique integer to each composer name\n",
        "# Source: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)  # Integer labels: [0, 1, 2, ...]\n",
        "\n",
        "# Convert integer labels into one-hot vectors\n",
        "# This is required by softmax output and categorical_crossentropy loss\n",
        "# Source: https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical\n",
        "y_onehot = to_categorical(y_encoded)\n",
        "\n",
        "# You can also access the class names like this:\n",
        "# class_names = label_encoder.classes_\n",
        "\n",
        "# Now y_onehot is your final target array for model training\n",
        "# Each label is a vector of length equal to the number of composers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5922ce9d-3f34-40aa-a11e-146d4d0cb350",
      "metadata": {
        "id": "5922ce9d-3f34-40aa-a11e-146d4d0cb350"
      },
      "outputs": [],
      "source": [
        "# First, convert X to a NumPy array of objects (since tensors may vary in shape)\n",
        "X_padded = pad_sequences(X, maxlen=4000, padding='post', dtype='float32')\n",
        "\n",
        "print(X_padded.shape)  # Should be (num_samples, max_time, pitch_bins, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d21faa-fd58-4a13-9b0c-3d5cb319c0a5",
      "metadata": {
        "id": "a6d21faa-fd58-4a13-9b0c-3d5cb319c0a5"
      },
      "outputs": [],
      "source": [
        "# After padding sequences, expand the tensor to add a second spatial dimension\n",
        "# Original shape: (samples, time_steps, pitch_bins, 1)\n",
        "# New shape:      (samples, time_steps, pitch_bins, 1, 1)\n",
        "# This ensures compatibility with Conv2D inside TimeDistributed\n",
        "X_reshaped = np.expand_dims(X_padded, axis=-1)\n",
        "\n",
        "# Split the reshaped input data and one-hot encoded labels into training and validation sets\n",
        "# - 'stratify': ensures class distribution stays balanced across both sets\n",
        "# - 'test_size': allocates 20% of data for validation\n",
        "# - 'random_state': ensures reproducibility of the split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_reshaped, y_onehot,\n",
        "    stratify=y_encoded,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Extract input dimensions from a single training sample\n",
        "# These dimensions are passed into the model to define the input layer shape\n",
        "time_dim, pitch_dim, width_dim, channel_dim = X_train.shape[1:]\n",
        "\n",
        "# Build the CNN→LSTM hybrid model using the computed input shape and number of classes\n",
        "# - 'input_shape': defines the structure of each input sample (per time step frame)\n",
        "# - 'num_classes': sets the output dimension for softmax classification\n",
        "model = build_cnn_lstm_model(\n",
        "    input_shape=(time_dim, pitch_dim, width_dim, channel_dim),\n",
        "    num_classes=len(label_encoder.classes_)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd7cfeef-963d-43a0-95ce-893ea1c11d25",
      "metadata": {
        "id": "cd7cfeef-963d-43a0-95ce-893ea1c11d25"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "716f9c9f-468d-4fd7-a403-3cc3971f0881",
      "metadata": {
        "id": "716f9c9f-468d-4fd7-a403-3cc3971f0881"
      },
      "source": [
        "### **Model Architecture Explained in Plain Terms**\n",
        "\n",
        "This model is built to listen to music (as a piano roll) and figure out which composer might have written it. It does this step by step, analyzing each slice of the music and learning how musical ideas evolve over time.\n",
        "\n",
        "#### **Layer Breakdown and Their Roles**\n",
        "\n",
        "| **Layer**                  | **What It Means (In Plain Terms)**                                                                 |\n",
        "|---------------------------|-----------------------------------------------------------------------------------------------------|\n",
        "| `TimeDistributed(Conv2D)` | Treats each moment of the music like a mini image and applies 32 pattern detectors to find motifs. It scans vertically across pitches to detect things like chords, intervals, or melodic contours. |\n",
        "| `MaxPooling2D((2,1))`     | Simplifies the detected patterns by reducing pitch resolution—like shrinking the image—while keeping essential information. This helps the model focus on broad musical shapes. |\n",
        "| `Flatten`                 | Converts each compressed musical slice into a row of numbers that the model can understand and learn from. This step prepares each moment for sequence learning. |\n",
        "| `LSTM(128)`               | Acts like musical memory—it learns how these patterns change over time, allowing the model to recognize phrasing, repetition, or rhythmic flow across the piece. |\n",
        "| `Dropout(0.3)`            | A regularization trick that randomly turns off some neurons during training to prevent the model from memorizing noise. Think of it like practicing blindfolded to improve intuition. |\n",
        "| `Dense(64)`               | Combines learned ideas into higher-level insights before making a prediction. It’s a final interpretation layer that distills what's been learned. |\n",
        "| `Dense(9)`                | Outputs 9 numbers that represent how likely the piece belongs to each composer. The highest number points to the model’s best guess. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Additional Stats and What They Tell Us**\n",
        "\n",
        "- **Parameter Count**: ~747,000 → This is how many things the model can tune as it learns. It's efficient enough to train quickly but expressive enough to capture real musical structure.\n",
        "- **Features After Flattening**: 1,312 values per moment in time → These represent all the musical information pulled from the convolutional layers, giving the LSTM plenty of texture to learn from.\n",
        "- **Output Shape**: `(None, 9)` → For every music sample, the model gives a vector of 9 scores, one for each composer. It picks the one with the highest score as its prediction.\n",
        "\n",
        "---\n",
        "\n",
        "This architecture combines visual pattern recognition (like reading sheet music) with temporal listening (like following a melody), making it especially well-suited for composer classification tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4794009c-7293-45c5-b681-d247a6a830fd",
      "metadata": {
        "id": "4794009c-7293-45c5-b681-d247a6a830fd"
      },
      "source": [
        "### 5. Model Training: Train the deep learning model using the pre-processed and feature-extracted data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3d20ae-deec-4131-9643-39aeba03d537",
      "metadata": {
        "id": "1a3d20ae-deec-4131-9643-39aeba03d537"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# TRAINING SETUP WITH CONFIGURABLE FEATURES\n",
        "# ==========================================\n",
        "\n",
        "# Toggle switches for enabling/disabling optional training features\n",
        "USE_GPU_CHECK        = True   # Check for GPU and report availability\n",
        "USE_MIXED_PRECISION  = True   # Use float16 operations to boost training speed on compatible GPUs\n",
        "USE_TENSORBOARD      = True   # Log training metrics and histograms for monitoring in TensorBoard\n",
        "USE_LR_SCHEDULER     = True   # Adjust learning rate dynamically to improve convergence and stability\n",
        "AUTO_SCALE_BATCH     = True   # Increase batch size automatically if GPU is available\n",
        "\n",
        "# ------------------------------------------\n",
        "# Device Check: Report available GPUs\n",
        "# ------------------------------------------\n",
        "if USE_GPU_CHECK:\n",
        "    from tensorflow.config import list_physical_devices\n",
        "    gpus = list_physical_devices('GPU')\n",
        "    print(\"Available GPUs:\", gpus)\n",
        "    if not gpus:\n",
        "        print(\"⚠️ No GPU detected — training will run on CPU\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# Mixed Precision: Enable float16 operations\n",
        "# Reduces memory footprint and speeds up computation\n",
        "# Recommended for Volta/Turing/Ampere GPUs (e.g. RTX series)\n",
        "# ------------------------------------------\n",
        "if USE_MIXED_PRECISION:\n",
        "    from tensorflow.keras import mixed_precision\n",
        "    mixed_precision.set_global_policy('mixed_float16')\n",
        "    print(\"Mixed precision policy:\", mixed_precision.global_policy())\n",
        "\n",
        "# ------------------------------------------\n",
        "# TensorBoard: Real-time training dashboard\n",
        "# View metrics at http://localhost:6006 after running:\n",
        "# tensorboard --logdir logs/fit/\n",
        "# ------------------------------------------\n",
        "if USE_TENSORBOARD:\n",
        "    import datetime\n",
        "    from tensorflow.keras.callbacks import TensorBoard\n",
        "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_cb = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "    print(\"TensorBoard logging to:\", log_dir)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Learning Rate Scheduler: Reduce LR every 10 epochs\n",
        "# Helps model fine-tune toward convergence after initial progress\n",
        "# ------------------------------------------\n",
        "if USE_LR_SCHEDULER:\n",
        "    from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "    def lr_schedule(epoch, lr):\n",
        "        if epoch % 10 == 0 and epoch != 0:\n",
        "            return lr * 0.5\n",
        "        return lr\n",
        "    lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Core Keras Callbacks\n",
        "# EarlyStopping prevents wasteful training beyond optimal point\n",
        "# ModelCheckpoint saves best weights based on validation performance\n",
        "# ------------------------------------------\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',           # Monitor validation loss to prevent overfitting\n",
        "    patience=5,                   # Wait up to 5 epochs for improvement\n",
        "    restore_best_weights=True     # Revert to best weights when training ends\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath='cnn_lstm_best_model.h5',  # Path to save best model\n",
        "    save_best_only=True                 # Save only if model improves\n",
        ")\n",
        "\n",
        "# ------------------------------------------\n",
        "# Auto-scaling batch size based on GPU availability\n",
        "# Larger batches improve training throughput when enough memory is available\n",
        "# ------------------------------------------\n",
        "batch_size = 32\n",
        "if AUTO_SCALE_BATCH and USE_GPU_CHECK and gpus:\n",
        "    batch_size = 64\n",
        "    print(\"Batch size auto-scaled to:\", batch_size)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Assemble final callback list based on active features\n",
        "# ------------------------------------------\n",
        "callbacks = [early_stop, checkpoint]\n",
        "if USE_TENSORBOARD:\n",
        "    callbacks.append(tensorboard_cb)\n",
        "if USE_LR_SCHEDULER:\n",
        "    callbacks.append(lr_scheduler)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Model Training: Fit CNN→LSTM on piano roll tensors\n",
        "# ------------------------------------------\n",
        "# - 'epochs': upper bound for training duration\n",
        "# - 'batch_size': number of samples used per training step\n",
        "# - 'validation_data': separate dataset used to evaluate generalization\n",
        "# - 'callbacks': applies monitoring, logging, regularization, and adaptive control\n",
        "# - 'verbose': controls verbosity of training output (1 = per epoch update)\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=30,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34bccd0a-cfcb-4ab6-8da6-3c157ccdea48",
      "metadata": {
        "id": "34bccd0a-cfcb-4ab6-8da6-3c157ccdea48"
      },
      "outputs": [],
      "source": [
        "tensorboard --logdir=logs/fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c99b7db5-47df-48f1-b176-ddf72739e8dd",
      "metadata": {
        "id": "c99b7db5-47df-48f1-b176-ddf72739e8dd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('CNN→LSTM Training Curve')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2326ad11-351f-4ff5-a9bf-eaa18efa8ae9",
      "metadata": {
        "id": "2326ad11-351f-4ff5-a9bf-eaa18efa8ae9"
      },
      "source": [
        "### 6. Model Evaluation: Evaluate the performance of the deep learning model using accuracy, precision, and recall metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5e106c9-22ab-4c12-94e4-71c0cd66c108",
      "metadata": {
        "id": "a5e106c9-22ab-4c12-94e4-71c0cd66c108"
      },
      "source": [
        "### 7. Model Optimization: Optimize the deep learning model by fine-tuning hyperparameters."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}